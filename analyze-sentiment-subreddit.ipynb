{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\" id=\"heading\">Sentiment Analysis of Reddit Data using Reddit API</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this live coding session, we leverage the Python Reddit API Wrapper (`PRAW`) to retrieve data from subreddits on [Reddit](https://www.reddit.com), and perform sentiment analysis using [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines) from [HuggingFace ( 🤗 the GitHub of Machine Learning )](https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/), powered by [transformer](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the session, you will "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- know how to work with APIs\n",
    "- feel more comfortable navigating thru documentation, even inspecting the source code\n",
    "- understand what a `pipeline` object is in HuggingFace\n",
    "- perform sentiment analysis using `pipeline`\n",
    "- run a python script in command line and get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of each task, commit* the work into the repository you created before the assignment\n",
    "- After completing all three tasks, make sure to push the notebook containing all code blocks and output cells to your repository you created before the assignment\n",
    "- Submit the link to the notebook in Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\***NEVER** commit a notebook displaying errors unless it is instructed otherwise. However, commit often; recall git ABC = **A**lways **B**e **C**ommitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task I: Instantiate a Reddit API Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first task is to instantiate a Reddit API object using [PRAW](https://praw.readthedocs.io/en/stable/), through which you will retrieve data. PRAW is a wrapper for [Reddit API](https://www.reddit.com/dev/api) that makes interacting with the Reddit API easier unless you are already an expert of [`requests`](https://docs.python-requests.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please ensure you've ran all the cells in the `imports.ipynb`, located [here](https://github.com/FourthBrain/MLE-8/blob/main/assignments/week-3-analyze-sentiment-subreddit/imports.ipynb), to make sure you have all the required packages for today's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  2. Create a new app on Reddit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a new app on Reddit and save secret tokens; refer to [post in medium](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Create a Reddit account if you don't have one, log into your account.\n",
    "- To access the API, we need create an app. Slight updates, on the website, you need to navigate to `preference` > `app`, or click [this link](https://www.reddit.com/prefs/apps) and scroll all the way down. \n",
    "- Click to create a new app, fill in the **name**, choose `script`, fill in  **description** and **redirect uri** ( The redirect URI is where the user is sent after they've granted OAuth access to your application (more info [here](https://github.com/reddit-archive/reddit/wiki/OAuth2)) For our purpose, you can enter some random url, e.g., www.google.com; as shown below.\n",
    "\n",
    "\n",
    "    <img src=\"https://miro.medium.com/max/700/1*lRBvxpIe8J2nZYJ6ucMgHA.png\" width=\"500\"/>\n",
    "- Jot down `client_id` (left upper corner) and `client_secret` \n",
    "\n",
    "    NOTE: CLIENT_ID refers to 'personal use script\" and CLIENT_SECRET to secret.\n",
    "    \n",
    "    <div>\n",
    "    <img src=\"https://miro.medium.com/max/700/1*7cGAKth1PMrEf2sHcQWPoA.png\" width=\"300\"/>\n",
    "    </div>\n",
    "\n",
    "- Create `secrets_reddit.py` in the same directory with this notebook, fill in `client_id` and `secret_id` obtained from the last step. We will need to import those constants in the next step.\n",
    "    ```\n",
    "    REDDIT_API_CLIENT_ID = \"client_id\"\n",
    "    REDDIT_API_CLIENT_SECRET = \"secret_id\"\n",
    "    REDDIT_API_USER_AGENT = \"any string except bot; ex. My User Agent\"\n",
    "    ```\n",
    "- Add `secrets_reddit.py` to your `.gitignore` file if not already done. NEVER push credentials to a repo, private or public. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Instantiate a `Reddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you are ready to create a read-only `Reddit` instance. Refer to [documentation](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html) when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import praw\n",
    "\n",
    "#import secrets_reddit\n",
    "\n",
    "# Create a Reddit object which allows us to interact with the Reddit API\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = \"euo_gpXfRJt-OYkJM1ixWw\",\n",
    "    client_secret = \"4rP6bJE6meTY4z8qoi7qSDtrf4_1Fw\",\n",
    "    user_agent = \"FourthBrain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.reddit.Reddit object at 0x7f8f6f940e80>\n"
     ]
    }
   ],
   "source": [
    "print(reddit) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "```<praw.reddit.Reddit object at 0x10f8a0ac0>```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Instantiate a `subreddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lastly, create a `subreddit` object for your favorite subreddit and inspect the object. The expected outputs you will see are from `r/machinelearning` unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Look at Obtain a subreddit section in URL\n",
    "# - https://praw.readthedocs.io/en/stable/getting_started/quick_start.html\n",
    "\n",
    "#create a subreddit object from Machine Learning \n",
    "sub_red = reddit.subreddit('machinelearning')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the display name of the subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machinelearning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(sub_red.display_name)\n",
    "\n",
    "# Output: MachineLearning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    machinelearning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How about its title, is it different from the display name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sub_red.title)\n",
    "\n",
    "# Output: Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    Machine Learning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Print out the description of the subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
      "--------\n",
      "+[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
      "--------\n",
      "+[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
      "--------\n",
      "+[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sub_red.description[:400])\n",
    "\n",
    "# Output: prints the first 400 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>\n",
    "\n",
    "    **[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
    "    --------\n",
    "    +[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
    "    --------\n",
    "    +[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
    "    --------\n",
    "    +[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
    "    --------\n",
    "    +[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task II: Parse comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1. Top Posts of All Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Find titles of top 10 posts of **all time** from your favorite subreddit. Refer to [Obtain Submission Instances from a Subreddit Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html)) if necessary. Verify if the titles match what you read on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# try run this line, what do you see? press q once you are done\n",
    "?sub_red.top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Project] From books to presentations in 10s with AR + ML\n",
      "[D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
      "[R] First Order Motion Model applied to animate paintings\n",
      "[N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
      "[D] This AI reveals how much time politicians stare at their phone at work\n",
      "[D] Types of Machine Learning Papers\n",
      "[D] The machine learning community has a toxicity problem\n",
      "I made a robot that punishes me if it detects that if I am procrastinating on my assignments [P]\n",
      "[Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
      "[P] Using oil portraits and First Order Model to bring the paintings back to life\n"
     ]
    }
   ],
   "source": [
    "# top ten posts of all time \n",
    "\n",
    "for submission in sub_red.top(time_filter ='all', limit = 10):\n",
    "    print(submission.title)\n",
    "\n",
    "# Output: 10 submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.models.comment_forest.CommentForest object at 0x7f8f6fe3f640>\n",
      "[D] Simple Questions Thread\n",
      "4\n",
      "wzxike\n",
      "https://www.reddit.com/r/MachineLearning/comments/wzxike/d_simple_questions_thread/\n",
      "<praw.models.comment_forest.CommentForest object at 0x7f8f701a0310>\n",
      "[D] Machine Learning - WAYR (What Are You Reading) - Week 140\n",
      "127\n",
      "vg5kjd\n",
      "https://www.reddit.com/r/MachineLearning/comments/vg5kjd/d_machine_learning_wayr_what_are_you_reading_week/\n",
      "<praw.models.comment_forest.CommentForest object at 0x7f8f707b0040>\n",
      "[D] \"A majority of respondents think that the scientific value of the majority of work in NLP is dubious\"\n",
      "118\n",
      "x0lz2f\n",
      "https://www.reddit.com/r/MachineLearning/comments/x0lz2f/d_a_majority_of_respondents_think_that_the/\n",
      "<praw.models.comment_forest.CommentForest object at 0x7f8f707b0460>\n",
      "[D] Easily Run Stable Diffusion Image to Image mode\n",
      "73\n",
      "x0jo8h\n",
      "https://www.reddit.com/r/MachineLearning/comments/x0jo8h/d_easily_run_stable_diffusion_image_to_image_mode/\n",
      "<praw.models.comment_forest.CommentForest object at 0x7f8f707c4790>\n",
      "[D] ML integration into marketing department. Any data scraping tools that can help with the processes?\n",
      "9\n",
      "x0u9b8\n",
      "https://www.reddit.com/r/MachineLearning/comments/x0u9b8/d_ml_integration_into_marketing_department_any/\n",
      "<praw.models.comment_forest.CommentForest object at 0x7f8f707c4640>\n",
      "[P] Model explainability for 🤗 Diffusers. Get explanations for your generated images with the latest stable diffusion!\n",
      "6\n",
      "x0tvut\n",
      "https://www.reddit.com/r/MachineLearning/comments/x0tvut/p_model_explainability_for_diffusers_get/\n",
      "<praw.models.comment_forest.CommentForest object at 0x7f8f707c41c0>\n",
      "[D] What are some dead ideas in machine learning or machine learning textbooks?\n",
      "208\n",
      "x05d1e\n",
      "https://www.reddit.com/r/MachineLearning/comments/x05d1e/d_what_are_some_dead_ideas_in_machine_learning_or/\n",
      "<praw.models.comment_forest.CommentForest object at 0x7f8f707c49a0>\n",
      "[R] Emotion-based symbolic music (MIDI) generation\n",
      "8\n",
      "x0oqtu\n",
      "https://www.reddit.com/r/MachineLearning/comments/x0oqtu/r_emotionbased_symbolic_music_midi_generation/\n",
      "<praw.models.comment_forest.CommentForest object at 0x7f8f6fe7d7c0>\n",
      "[R] Novel Methods for Drift Detection\n",
      "7\n",
      "x0n6fg\n",
      "https://www.reddit.com/r/MachineLearning/comments/x0n6fg/r_novel_methods_for_drift_detection/\n",
      "<praw.models.comment_forest.CommentForest object at 0x7f8f6fda4550>\n",
      "[D] Weird consequence of not freezing layers in Neural Network\n",
      "1\n",
      "x0tlab\n",
      "https://www.reddit.com/r/MachineLearning/comments/x0tlab/d_weird_consequence_of_not_freezing_layers_in/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# assume you have a Subreddit instance bound to variable `subreddit`\n",
    "for submission in sub_red.hot(limit=10):\n",
    "    print(submission.comments)\n",
    "    print(submission.title)\n",
    "    # Output: the submission's title\n",
    "    print(submission.score)\n",
    "    # Output: the submission's score\n",
    "    print(submission.id)\n",
    "    # Output: the submission's ID\n",
    "    print(submission.url)\n",
    "    # Output: the URL the submission points to or the submission's URL if it's a self post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details> <summary>Expected output:</summary>\n",
    "\n",
    "    [Project] From books to presentations in 10s with AR + ML\n",
    "    [D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
    "    [R] First Order Motion Model applied to animate paintings\n",
    "    [N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
    "    [D] This AI reveals how much time politicians stare at their phone at work\n",
    "    [D] Types of Machine Learning Papers\n",
    "    [D] The machine learning community has a toxicity problem\n",
    "    [Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
    "    [P] Using oil portraits and First Order Model to bring the paintings back to life\n",
    "    [D] Convolution Neural Network Visualization - Made with Unity 3D and lots of Code / source - stefsietz (IG)    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2. Top 10 Posts of This Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are the titles of the top 10 posts of **this week** from your favorite subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P] Run Stable Diffusion locally with a web UI + artist workflow video\n",
      "[D] StableDiffusion v1.4 is entirely public. What do you think about Stability.ai ?\n",
      "[P] Einstein Instant NeRF\n",
      "[D] How to Run Stable Diffusion (Locally and in Colab)\n",
      "[D][N]\"Mudge learned that Twitter had never acquired proper legal rights to training material used to build Twitter's key Machine Learning models. The Machine Learning models at issue were some of the core models running the company's most basic products, like which Tweets to show each user.\"\n",
      "[D] ML for Good\n",
      "[D] What are some dead ideas in machine learning or machine learning textbooks?\n",
      "[P] Run stable diffusion in google colab including image2image and inpainting\n",
      "[D] A thought I had on Yann LeCun's recent paper \"A Path Towards Autonomous Machine Intelligence\"\n",
      "[D] \"A majority of respondents think that the scientific value of the majority of work in NLP is dubious\"\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# top ten posts of THIS WEEK \n",
    "\n",
    "for submission in sub_red.top(time_filter ='week', limit = 10):\n",
    "    print(submission.title)\n",
    "\n",
    "# Output: 10 submissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details><summary>Expected output:</summary>\n",
    "\n",
    "    [N] Ian Goodfellow, Apple’s director of machine learning, is leaving the company due to its return to work policy. In a note to staff, he said “I believe strongly that more flexibility would have been the best policy for my team.” He was likely the company’s most cited ML expert.\n",
    "    [R][P] Thin-Plate Spline Motion Model for Image Animation + Gradio Web Demo\n",
    "    [P] I’ve been trying to understand the limits of some of the available machine learning models out there. Built an app that lets you try a mix of CLIP from Open AI + Apple’s version of MobileNet, and more directly on your phone's camera roll.\n",
    "    [R] Meta is releasing a 175B parameter language model\n",
    "    [N] Hugging Face raised $100M at $2B to double down on community, open-source & ethics\n",
    "    [P] T-SNE to view and order your Spotify tracks\n",
    "    [D] : HELP Finding a Book - A book written for Google Engineers about foundational Math to support ML\n",
    "    [R] Scaled up CLIP-like model (~2B) shows 86% Zero-shot on Imagenet\n",
    "    [D] Do you use NLTK or Spacy for text preprocessing?\n",
    "    [D] Democratizing Diffusion Models - LDMs: High-Resolution Image Synthesis with Latent Diffusion Models, a 5-minute paper summary by Casual GAN Papers\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "Check out what other attributes the `praw.models.Submission` class has in the [docs](https://praw.readthedocs.io/en/stable/code_overview/models/submission.html). \n",
    "\n",
    "1. After having a chance to look through the docs, is there any other information that you might want to extract? How might this additional data help you?\n",
    "\n",
    "Yes, score, ID, and url are some are other information that can be extracted. There are many more pieces of info tha can be extracted. This information gives you information on the post which can be used to analysis or otherwise. Other information that can be extracted can be found in this url: https://praw.readthedocs.io/en/stable/code_overview/models/submission.html\n",
    "\n",
    "\n",
    "\n",
    "Write a sample piece of code below extracting three additional pieces of information from the submission below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1195\n",
      "wz68mz\n",
      "https://v.redd.it/djdpfsmy2ak91\n",
      "416\n",
      "wv50uh\n",
      "https://www.reddit.com/r/MachineLearning/comments/wv50uh/d_stablediffusion_v14_is_entirely_public_what_do/\n",
      "336\n",
      "wzmokl\n",
      "https://v.redd.it/jacxo1lgvdk91\n",
      "263\n",
      "wvr23n\n",
      "https://www.reddit.com/r/MachineLearning/comments/wvr23n/d_how_to_run_stable_diffusion_locally_and_in_colab/\n",
      "230\n",
      "wx7rw5\n",
      "https://www.reddit.com/r/MachineLearning/comments/wx7rw5/dnmudge_learned_that_twitter_had_never_acquired/\n",
      "220\n",
      "wwdqp8\n",
      "https://www.reddit.com/r/MachineLearning/comments/wwdqp8/d_ml_for_good/\n",
      "214\n",
      "x05d1e\n",
      "https://www.reddit.com/r/MachineLearning/comments/x05d1e/d_what_are_some_dead_ideas_in_machine_learning_or/\n",
      "124\n",
      "wxogf0\n",
      "https://www.reddit.com/r/MachineLearning/comments/wxogf0/p_run_stable_diffusion_in_google_colab_including/\n",
      "119\n",
      "x0lz2f\n",
      "https://www.reddit.com/r/MachineLearning/comments/x0lz2f/d_a_majority_of_respondents_think_that_the/\n",
      "114\n",
      "wyqyu8\n",
      "https://www.reddit.com/r/MachineLearning/comments/wyqyu8/d_a_thought_i_had_on_yann_lecuns_recent_paper_a/\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "for submission in sub_red.top(time_filter ='week', limit = 10):\n",
    "    print(submission.score)\n",
    "    # Output: the submission's score\n",
    "    print(submission.id)\n",
    "    # Output: the submission's ID\n",
    "    print(submission.url)\n",
    "    # Output: the URL the submission points to or the submission's URL if it's a self post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "2. Is there any information available that might be a concern when it comes to Ethical Data?\n",
    "\n",
    "It seems most of this information does not compromise a person's personal information. However, there are other things to consider such as over_18 and nsfw, which essentially tag content as mature or offensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3. Comment Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add comments to the code block below to describe what each line of the code does (Refer to [Obtain Comment Instances Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html) when necessary). The code is adapted from [this tutorial](https://praw.readthedocs.io/en/stable/tutorials/comments.html)\n",
    "\n",
    "The purpose is \n",
    "1. to understand what the code is doing \n",
    "2. start to comment your code whenever it is not self-explantory if you have not (others will thank you, YOU will thank you later 😊) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 470 ms, sys: 45.5 ms, total: 515 ms\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "\n",
    "# this creates an empty list in which comments will appended into\n",
    "top_comments = []\n",
    "\n",
    "\n",
    "#loops through the top ten posts \n",
    "for submission in sub_red.top(limit=10):\n",
    "    # if top_level-coment is an instance of MoreComments\n",
    "    #this submission's comment forest contains a number of MoreComments objects \n",
    "    # these objects represent the \"load more comments\" and \"continue this thread\"\n",
    "    for top_level_comment in submission.comments:\n",
    "        # isinstance() checks if top_level_comment is an instance of MoreComments\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        # append body of top_level_comment to list\n",
    "        top_comments.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4. Inspect Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How many comments did you extract from the last step? Examine a few comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of top comments: 741\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE  # the answer may vary 693 for r/machinelearning\n",
    "\n",
    "print(f\"Number of top comments: {len(top_comments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Every data scientist today is truly standing on the shoulders of giants.',\n",
       " \"What are the edges cases when this doesn't work? Does this require certain lighting conditions etc? How does it know to extract both people from the image?\",\n",
       " 'Is there a hacky way I could modify this to work with streaming audio from a place like spotify, or would it require a pretty big overhaul of the code? Does anyone know?',\n",
       " \"There's no way that took 10s to develop, install, try and record an 57 sec video of. I mean, yeah, technology and stuff, but not in 10s. Sorry.\",\n",
       " \"Does that mean we're supposed to find and share the link? Come on, op....\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "[random.choice(top_comments) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> <summary>Some of the comments from `r/machinelearning` subreddit are:</summary>\n",
    "\n",
    "    ['Awesome visualisation',\n",
    "    'Similar to a stack or connected neurons.',\n",
    "    'Will this Turing pass the Turing Test?']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "3. After having a chance to review a few samples of 5 comments from the subreddit, what can you say about the data? \n",
    "\n",
    "The data varies in length, there may be mispellings, punctuaction, etc. The data is not \"clean\". \n",
    "\n",
    "HINT: Think about the \"cleanliness\" of the data, the content of the data, think about what you're trying to do - how does this data line up with your goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 5. Extract Top Level Comment from Subreddit `TSLA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write your code to extract top level comments from the top 10 topics of a time period, e.g., year, from subreddit `TSLA` and store them in a list `top_comments_tsla`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "top_comments_tsla =[]\n",
    "subreddit = reddit.subreddit('TSLA')\n",
    "top_comments = []\n",
    "\n",
    "#loops through the top ten posts \n",
    "for submission in subreddit.top(limit=10):\n",
    "    # if top_level-coment is an instance of MoreComments\n",
    "    #this submission's comment forest contains a number of MoreComments objects \n",
    "    # these objects represent the \"load more comments\" and \"continue this thread\"\n",
    "    for top_level_comment in submission.comments:\n",
    "        # isinstance() checks if top_level_comment is an instance of MoreComments\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        # append body of top_level_comment to list\n",
    "        top_comments_tsla.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_comments_tsla) # Expected: 174 for r/machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here is a very interesting theory/conspiracy https://twitter.com/robgrav3s/status/1461111661310398469?s=10',\n",
       " '[removed]',\n",
       " 'Or you are actually an insider and freaked out that the post got attention lol. \\n\\nPm me with the next hot tip lol']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random.choice(top_comments_tsla) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Some of the comments from `r/TSLA` subreddit:</summary>\n",
    "\n",
    "    ['I bought puts',\n",
    "    '100%',\n",
    "    'Yes. And I’m bag holding 1200 calls for Friday and am close to throwing myself out the window']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "4. Now that you've had a chance to review another subreddits comments, do you see any differences in the kinds of comments either subreddit has - and how might this relate to bias?\n",
    "\n",
    "- Depending on which subreddit you are examining there will be some kind of bias. For example, if you look at the Republican and Democrat subreddits, there will be a bias towards right and left views, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task III: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us analyze the sentiment of comments scraped from `r/TSLA` using a pre-trained HuggingFace model to make the inference. Take a [Quick tour](https://huggingface.co/docs/transformers/quicktour). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Import `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Create a Pipeline to Perform Task \"sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Get one comment from list `top_comments_tsla` from Task II - 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comment = random.choice(top_comments_tsla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Are you legally allowed to share that information though before they announced it?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example comment is: `'Bury Burry!!!!!'`. Print out what you get. For reproducibility, use the same comment in the next step; consider setting a seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Make Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentiment = sentiment_model([comment])# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the type of the output `sentiment`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9689354300498962}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "A list of dictionaries \n",
    "\n",
    "Negative 0.969\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment: Are you legally allowed to share that information though before they announced it?\n",
      "Predicted Label is NEGATIVE and the score is 0.969\n"
     ]
    }
   ],
   "source": [
    "print(f'The comment: {comment}')\n",
    "print(f'Predicted Label is {sentiment[0][\"label\"]} and the score is {sentiment[0][\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example comment, the output is:\n",
    "\n",
    "    The comment: Bury Burry!!!!!\n",
    "    Predicted Label is NEGATIVE and the score is 0.989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🖥️❓ Model Question:\n",
    "\n",
    "1. What does the score represent?\n",
    "\n",
    "The score is the confidence of the model. It is 96.9% confident the sentiment is Negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task IV: Put All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull all the piece together, create a simple script that does \n",
    "\n",
    "- get the subreddit\n",
    "- get comments from the top posts for given subreddit\n",
    "- run sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the code, running the following block writes the code into a new Python script and saves it as `top_tlsa_comment_sentiment.py` under the same directory with the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_tlsa_comment_sentiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_tlsa_comment_sentiment.py\n",
    "\n",
    "import secrets_reddit as sr\n",
    "import random\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from praw import Reddit\n",
    "from praw.models.reddit.subreddit import Subreddit\n",
    "from praw.models import MoreComments\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_subreddit(display_name:str) -> Subreddit:\n",
    "    \"\"\"Get subreddit object from display name\n",
    "\n",
    "    Args:\n",
    "        display_name (str): [description]\n",
    "\n",
    "    Returns:\n",
    "        Subreddit: [description]\n",
    "    \"\"\"\n",
    "    reddit = Reddit(\n",
    "        client_id = \"euo_gpXfRJt-OYkJM1ixWw\",\n",
    "        client_secret = \"4rP6bJE6meTY4z8qoi7qSDtrf4_1Fw\",\n",
    "        user_agent = \"FourthBrain\"\n",
    "        )\n",
    "    \n",
    "    subreddit = reddit.subreddit(display_name)\n",
    "    return subreddit\n",
    "\n",
    "def get_comments(subreddit:Subreddit, limit:int=3) -> List[str]:\n",
    "    \"\"\" Get comments from subreddit\n",
    "\n",
    "    Args:\n",
    "        subreddit (Subreddit): [description]\n",
    "        limit (int, optional): [description]. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of comments\n",
    "    \"\"\"\n",
    "    top_comments = []\n",
    "    for submission in subreddit.top(limit=limit):\n",
    "        for top_level_comment in submission.comments:\n",
    "            if isinstance(top_level_comment, MoreComments):\n",
    "                continue\n",
    "            top_comments.append(top_level_comment.body)\n",
    "    return top_comments\n",
    "\n",
    "def run_sentiment_analysis(comment:str) -> Dict:\n",
    "    \"\"\"Run sentiment analysis on comment using default distilbert model\n",
    "    \n",
    "    Args:\n",
    "        comment (str): [description]\n",
    "        \n",
    "    Returns:\n",
    "        str: Sentiment analysis result\n",
    "    \"\"\"\n",
    "    sentiment_model = pipeline(\"sentiment-analysis\")\n",
    "    sentiment = sentiment_model(comment)\n",
    "    return sentiment[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    subreddit = get_subreddit('TSLA')# YOUR CODE HERE\n",
    "    comments = get_comments(subreddit)\n",
    "    comment = random.choice(comments)\n",
    "    sentiment = run_sentiment_analysis(comment)\n",
    "    \n",
    "    print(f'The comment: {comment}')\n",
    "    print(f'Predicted Label is {sentiment[\"label\"]} and the score is {sentiment[\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "The comment: So funny! Same thing can't buy anymore dips\n",
      "Predicted Label is POSITIVE and the score is 0.983\n"
     ]
    }
   ],
   "source": [
    "!python top_tlsa_comment_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> Expected output:</summary>\n",
    "\n",
    "    No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "    The comment: When is DOGE flying\n",
    "    Predicted Label is POSITIVE and the score is 0.689\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "5. Is the subreddit active? About how many posts or threads per day? How could you find this information?\n",
    "\n",
    "Yes, the subreddit is active. The code below prints the number of posts in the past day. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of posts per day is 3\n"
     ]
    }
   ],
   "source": [
    "# this prints the number of posts per day \n",
    "\n",
    "count = 0\n",
    "for submission in subreddit.top(time_filter ='day',limit=None):\n",
    "    count = count + 1\n",
    "\n",
    "print(f\"The number of posts per day is {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "6. Does there seem to be a large distribution of posters or a smaller concentration of posters who are very active? What kind of impact might this have on the data?\n",
    "\n",
    "From the coutput of the code below we can see that there is a large distributiion of posters. This means that the data comes from a lot of sources, making it more diverse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({Redditor(name='TSLAinsider'): 1,\n",
       "         Redditor(name='MrBills07'): 1,\n",
       "         Redditor(name='bigjimz88'): 3,\n",
       "         Redditor(name='G_Train24'): 1,\n",
       "         Redditor(name='lxelite89'): 1,\n",
       "         Redditor(name='ThenickThenick'): 1,\n",
       "         Redditor(name='Kornbelly'): 2,\n",
       "         Redditor(name='Kaiserschmorrn'): 2,\n",
       "         Redditor(name='_feelsgoodman__'): 1,\n",
       "         None: 4,\n",
       "         Redditor(name='Coffeeandtrade'): 1,\n",
       "         Redditor(name='jrow68'): 1,\n",
       "         Redditor(name='jrventure1'): 2,\n",
       "         Redditor(name='BelmontMan'): 1,\n",
       "         Redditor(name='_Dannyio'): 1,\n",
       "         Redditor(name='DopeJeprox007'): 1,\n",
       "         Redditor(name='Newt_Gold'): 1,\n",
       "         Redditor(name='unleashthepotential'): 1,\n",
       "         Redditor(name='ExplanationGeneral31'): 1,\n",
       "         Redditor(name='Savings-Bee-2896'): 2,\n",
       "         Redditor(name='SantiagoCoffee'): 2,\n",
       "         Redditor(name='droneauto'): 7,\n",
       "         Redditor(name='Legal_Philosopher_26'): 4,\n",
       "         Redditor(name='harlando-calrissian'): 1,\n",
       "         Redditor(name='Stonkboss1'): 1,\n",
       "         Redditor(name='S3bluen'): 1,\n",
       "         Redditor(name='Letstalkbull'): 1,\n",
       "         Redditor(name='Confident-Leopard-19'): 1,\n",
       "         Redditor(name='JeerFear'): 1,\n",
       "         Redditor(name='Wei_28'): 1,\n",
       "         Redditor(name='rcitigori'): 1,\n",
       "         Redditor(name='NeuralNetworth420'): 1,\n",
       "         Redditor(name='Responsible_Papaya93'): 1,\n",
       "         Redditor(name='Bm0ore'): 3,\n",
       "         Redditor(name='lilsstrue'): 1,\n",
       "         Redditor(name='ActualInvesting'): 1,\n",
       "         Redditor(name='jins505'): 1,\n",
       "         Redditor(name='Jizzyelk'): 2,\n",
       "         Redditor(name='LingonberryCalm4316'): 1,\n",
       "         Redditor(name='Puzzleheaded_Basil13'): 1,\n",
       "         Redditor(name='trowawayfarawaytoday'): 1,\n",
       "         Redditor(name='Lightspeed_3108'): 2,\n",
       "         Redditor(name='evdude83'): 1,\n",
       "         Redditor(name='Appropriate_Ninja_57'): 2,\n",
       "         Redditor(name='briggette1040'): 1,\n",
       "         Redditor(name='JohnnyCas2031'): 1,\n",
       "         Redditor(name='IhateFARTINGatWORK'): 1,\n",
       "         Redditor(name='wewewawa'): 2,\n",
       "         Redditor(name='MrAlucarD69'): 1,\n",
       "         Redditor(name='millionbulls'): 1,\n",
       "         Redditor(name='Mindless_Ad_4338'): 1,\n",
       "         Redditor(name='armadeooo'): 1,\n",
       "         Redditor(name='Lost-Guarantee229'): 1,\n",
       "         Redditor(name='space_s3x'): 1,\n",
       "         Redditor(name='ExuberantBadger'): 1,\n",
       "         Redditor(name='Bluecomet09'): 1,\n",
       "         Redditor(name='robtbo'): 1,\n",
       "         Redditor(name='AgentBroccoli'): 1,\n",
       "         Redditor(name='SnooOranges7510'): 1,\n",
       "         Redditor(name='Umaruchan1776'): 1,\n",
       "         Redditor(name='PaoloCadoni'): 1,\n",
       "         Redditor(name='crusaderx2'): 1,\n",
       "         Redditor(name='Green-dollar'): 1,\n",
       "         Redditor(name='alpha247365'): 1,\n",
       "         Redditor(name='drjnaqvi'): 1,\n",
       "         Redditor(name='JAYNEO2'): 1,\n",
       "         Redditor(name='Tsla-all-the-way'): 1,\n",
       "         Redditor(name='risitodeplata'): 1,\n",
       "         Redditor(name='Archi-SPARCHS-1234'): 1,\n",
       "         Redditor(name='clint9498paul'): 1,\n",
       "         Redditor(name='CWay76'): 1,\n",
       "         Redditor(name='Sealsasa'): 1,\n",
       "         Redditor(name='Gee_Dee_'): 1,\n",
       "         Redditor(name='Warren_MuffClit'): 1,\n",
       "         Redditor(name='ju-ju2020'): 1})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this counts the number of authors in the top 100 posts \n",
    "\n",
    "authors = []\n",
    "\n",
    "for submission in subreddit.top(time_filter ='all',limit=100):\n",
    "    #print(submission.author)\n",
    "    \n",
    "    authors.append(submission.author)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Counter(authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle-course",
   "language": "python",
   "name": "mle-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "c57794392b841cffd8686d5c4548e4e2ec78521f49300d60954d1380f1b4bd1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
